#!/usr/bin/env python3
"""
Script for basic statistical analysis of log data stored in Parquet format.
Reads a Parquet file generated by blocker.py --dump-data and prints insights.
"""
import argparse
import logging
import os
import sys
import ipaddress
import pandas as pd

# Attempt to import pyarrow, needed by pandas.read_parquet
try:
    import pyarrow
except ImportError:
    print("Error: 'pyarrow' library is required but not installed.", file=sys.stderr)
    print("Please install it: pip install pyarrow", file=sys.stderr)
    sys.exit(1)

# Logging configuration
LOG_FORMAT = '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
LOG_DATE_FORMAT = '%Y-%m-%d %H:%M:%S'

def setup_logging(log_level=logging.INFO):
    """Configure basic console logging."""
    logging.basicConfig(level=log_level, format=LOG_FORMAT, datefmt=LOG_DATE_FORMAT)

def get_subnet(ip_str, ipv4_prefix=24, ipv6_prefix=64):
    """Calculates the subnet for a given IP string."""
    try:
        ip_obj = ipaddress.ip_address(ip_str)
        if ip_obj.version == 4:
            return str(ipaddress.ip_network(f"{ip_str}/{ipv4_prefix}", strict=False))
        elif ip_obj.version == 6:
            return str(ipaddress.ip_network(f"{ip_str}/{ipv6_prefix}", strict=False))
    except ValueError:
        return None # Invalid IP string
    return None

def detect_ip_column(df_columns):
    """Detects the IP address column from a list of common names."""
    common_ip_cols = ['ip_address', 'ip', 'client_ip', 'remote_addr']
    for col in common_ip_cols:
        if col in df_columns:
            return col
    return None

def main():
    """Main execution function."""
    setup_logging()
    logger = logging.getLogger('log_analyzer')

    parser = argparse.ArgumentParser(
        description='Analyze log data from a Parquet file.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        '--file', '-f', required=True,
        help='Path to the Parquet log dump file to analyze.'
    )
    parser.add_argument(
        '--top', '-n', type=int, default=10,
        help='Number of top IPs/Subnets to display.'
    )
    # REMOVED --ip-column argument
    args = parser.parse_args()

    # --- File Validation ---
    if not os.path.exists(args.file):
        logger.error(f"Error: Parquet file not found: {args.file}")
        sys.exit(1)

    logger.info(f"Loading Parquet file: {args.file}")
    try:
        # Assuming the timestamp was saved as the index in blocker.py
        # If not, adjust 'index=...' or load and then set_index('timestamp_column_name')
        df = pd.read_parquet(args.file)
        # Check if index is DatetimeIndex, otherwise try to find a timestamp column
        if not isinstance(df.index, pd.DatetimeIndex):
             # Attempt to find a suitable timestamp column if index isn't it
             ts_cols = df.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns
             if 'timestamp' in df.columns and pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                 logger.info("Setting 'timestamp' column as DataFrame index.")
                 df = df.set_index('timestamp')
             elif ts_cols.any():
                 logger.warning(f"Using first datetime column '{ts_cols[0]}' as index. Verify this is correct.")
                 df = df.set_index(ts_cols[0])
             else:
                 logger.error("Could not find a suitable DatetimeIndex or timestamp column.")
                 sys.exit(1)

        # Ensure index is UTC if it has timezone, otherwise assume UTC if naive
        if df.index.tz is not None:
            df.index = df.index.tz_convert('UTC')
            logger.info("Converted index to UTC.")
        else:
            # If index is naive, assume it was originally UTC as stored by blocker.py
            df.index = df.index.tz_localize('UTC')
            logger.info("Localized naive index to UTC.")


        logger.info(f"Successfully loaded {len(df)} records.")
    except Exception as e:
        logger.error(f"Error reading Parquet file {args.file}: {e}", exc_info=True)
        sys.exit(1)

    if df.empty:
        logger.warning("The Parquet file is empty. No analysis to perform.")
        sys.exit(0)

    # --- Basic Analysis ---
    print("\n=== BASIC STATISTICS ===")
    total_requests = len(df)
    print(f"Total Requests: {total_requests}")

    if total_requests > 0:
        min_time = df.index.min()
        max_time = df.index.max()
        time_range = max_time - min_time
        print(f"Time Range:")
        print(f"  Start: {min_time}")
        print(f"  End:   {max_time}")
        print(f"  Duration: {time_range}")

        # --- IP Analysis ---
        # Detect IP column automatically
        ip_col_name = detect_ip_column(df.columns)

        if ip_col_name:
            logger.info(f"Detected IP address column: '{ip_col_name}'")
            unique_ips = df[ip_col_name].nunique()
            print(f"\nUnique IP Addresses: {unique_ips}")

            ip_counts = df[ip_col_name].value_counts()
            print(f"\nTop {args.top} IPs by Request Count:")
            print(ip_counts.head(args.top).to_string())

            # --- Subnet Analysis ---
            logger.info("Calculating subnets...")
            # Apply the get_subnet function using the detected column name
            df['subnet'] = df[ip_col_name].apply(get_subnet)
            unique_subnets = df['subnet'].nunique()
            print(f"\nUnique Subnets (/24 or /64): {unique_subnets}")

            subnet_counts = df['subnet'].value_counts()
            print(f"\nTop {args.top} Subnets by Request Count:")
            print(subnet_counts.head(args.top).to_string())
        else:
            # Update warning message
            logger.warning(f"Could not automatically detect an IP address column "
                           f"(checked for: ip_address, ip, client_ip, remote_addr). "
                           f"Skipping IP and Subnet analysis.")

        # --- Temporal Analysis ---
        print(f"\nRequests per Hour (UTC):")
        # Resample by hour and count occurrences
        hourly_counts = df.resample('h').size() # 'h' is deprecated, use 'H' or 'h'
        # Filter out hours with zero requests for brevity if desired
        hourly_counts = hourly_counts[hourly_counts > 0]
        if not hourly_counts.empty:
             print(hourly_counts.to_string())
        else:
             print("  No requests found in the time range.")

    print("\nAnalysis complete.")

if __name__ == '__main__':
    main()
